{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> University of Haifa - Computer Vision course\n",
    "\n",
    "\n",
    "## Homework 2 - 3D Vision\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: Thursday 12.6.2025 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**, unless you have already got an approval to work on your own\n",
    "\n",
    "* Submission is only through Moodle\n",
    "\n",
    "* You can choose your working environment:\n",
    "    * `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, but you will not need it for this excercise.\n",
    "    * Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "<div style=\"margin-top:6px;\"></div>\n",
    "\n",
    "* You should submit only this ipynb notebook file, **with all cells executed such that the results will be included**.\n",
    "<div style=\"margin-top:6px;\"></div>\n",
    "\n",
    "* Read the instructions carefully:\n",
    "    * You have functions to complete and scripts to write - look for <span style=\"color:red\">''' YOUR CODE HERE '''</span>\n",
    "        * Make sure to properly document your code\n",
    "        * Notice where you can use existing implementations (e.g. opencv) freely and in which cases you have to provide your own implementation of certain steps\n",
    "    * You have some explanations and discussions to write - look for <span style=\"color: red; font-weight: bold;\">YOUR EXPLANATION\\DISCUSSION HERE</span>\n",
    "        * Make sure to answer shortly but informatively\n",
    "<div style=\"margin-top:6px;\"></div>\n",
    "\n",
    "* Code of Honor:\n",
    "    * We wish to avoid dealing with copied homework. However, we will not hesitate to take serious actions against students that are caught with violations of the Honor Code. Note that in this course, it is rather easy to detect similar submissions, as most of them require your \"personal\" touch.     \n",
    "<div style=\"margin-top:6px;\"></div>\n",
    "\n",
    "* Submission date:\n",
    "    * Submission date of 12.6.2025 is final and will not be delayed\n",
    "    * However, you may submit up to 2 days late (till Sunday 14.6.2025) at the cost of 4 points per day\n",
    "    * Any (fully justified and documented) requests for delay must be sent till Sunday 8.6 at the latest\n",
    "    * The excercise is long and isn't easy - **make sure to start early**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* `scipy`\n",
    "* Anything else you need (`os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">IMPORTANT:</font> Make sure that each output image has a title to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Constraining putative matches using geometry\n",
    "---\n",
    "\n",
    "### Implement the functions below (separately), given a pair of images to be matched:\n",
    "\n",
    "## 1. Obtain SIFT-based matches\n",
    "* Compute SIFT features in each of the images\n",
    "* Perform SIFT matching between the two sets of features, to obtain a set of matches M\n",
    "\n",
    "## 2. Visualize a set of matches\n",
    "* Visualize the image pair, side-by-side, with thin lines connecting matching points (You may visualize a large random subset, if there are too many matches and the result is too cluttered)\n",
    "\n",
    "## 3. Recover the Epipolar geometry\n",
    "* Given the set of matches, estimate a Fundamental matrix F\n",
    "\n",
    "## 4. Visualize the Epipolar geometry\n",
    "* Given F, sample 10 2D points, uniformly at random, in image 1 and compute (using F), the epipolar lines in image 2.\n",
    "* Visualize the image pair, side-by-side. Choose 10 unique different colors (preferably spread uniformly over some colorspace) and then use colored markers to overlay the points in image 1 and the corresponding colors to overlay the epipolar lines in image 2.\n",
    "* Repeat the above, by switching roles (random points in image 2 and matching epipoles in image 1).\n",
    "\n",
    "## 5. Constrain a set of matches, by filtering them with the obtained geometry\n",
    "* Find and return the subset M' of the original set of matches M, that 'agree' with the obtained F, up to some error threshold.\n",
    "\n",
    "\n",
    "### Using the above functions, for each pair of images in data/pairs_to_match, perform the following:\n",
    "* Visualize a simple SIFT-based set of matches\n",
    "* Estimate the fundamental matrix F and visualize the epipolar geometry\n",
    "* Constrain the set of matches using F and visualize the result.\n",
    "* Do you observe that constraining the matches to the epipolar geometry (given by F) is helpful in any way? (Was it able to filter out all of the outliers? Did it filter out any inliers?). Explain in the second cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL OF YOUR CODE FOR PART 1 SHOULD BE IN THIS CELL\n",
    "\n",
    "# REQUIRED OUTPUTS (For each of the input stereo pairs in data/pairs_to_match):\n",
    "# 1. Visualize matches\n",
    "# 2. Visualize epipolar geometry\n",
    "# 3. Visualize matches after constraints\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-weight: bold;\">YOUR EXPLANATION HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Middle-View synthesis from a rectified stereo pair.\n",
    "---\n",
    "\n",
    "### In this section, given a *rectified* image pair, we wish to generate/synthesize the image that would have been captured by a camera situated exactly in the middle, between the pair of poses.\n",
    "\n",
    "## We will follow two different strategies to do so:\n",
    "* Strategy A: Via photometric stereo\n",
    "* Strategy B: Via sparse reconstruction\n",
    "\n",
    "## Details for Strategy A:\n",
    "* Compute a (dense) disparity map between the pair of images: **Implement your own (don't use a ready library function for this)**\n",
    "* Use this map to synthesize the 'central' image\n",
    "* Explain what problems arise in this approach and how your solution tries to deal with them\n",
    "\n",
    "## Details for Strategy B:\n",
    "* Use the code from the previous parts to obtain a set of matches - ones that obeys with the geometry. How would you do this even without knowing the fundamental matrix?\n",
    "* Using triangulation, find the locations of the interest points in the new 'central' image (In practice - this can be done by simply using the disparity for each match). At this point - You should have 'triplet' matches, each consisting of 1 point in each of the 3 viewpoints.\n",
    "* Take the set of (match) points in one of the images (left, right or middle - to your choice) and sub-divide the image into small triangular or quadrilateral sub-regions, whose vertices are placed on the matched interest points. This can be done, for example, by computing any kind of triangulation over the interest points (see the 'Dylan' example under the 'Examples' folder). Once obtaining the subdivision (e.g. triangulation), it can be transfered to the other two images, using the corresponding matching points.\n",
    "* For each region (triangle or quadrilateral), copy the contents from one of the input images to the target (middle) image, using either an affine transformation (for a triangle) or a homography (for a quadrilateral).\n",
    "* Explain what problems arise in this approach and how your solution tries to deal with them.\n",
    "\n",
    "## Implement your solutions and expain them in the following 4 cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL OF YOUR CODE FOR STATEGY A SHOULD BE IN THIS CELL\n",
    "\n",
    "# REQUIRED OUTPUTS (For each of the input stereo pairs in data/rectified_pairs):\n",
    "# 1. Visualize, side-by-side: left-image | right-image\n",
    "# 2. Visualize, side-by-side: left-disparity-image | right_disparity-image\n",
    "# 3. Visualize, by constructing an infinite-loop GIF of the four images: img_1->img_mid->img_2->img_mid->,\n",
    "#    using the following given function:\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "def create_and_display_gif(image_files, gif_path='animation.gif', duration=0.5):\n",
    "    \"\"\"\n",
    "    Creates a GIF from a list of image files and displays it in a Jupyter notebook.\n",
    "\n",
    "    Parameters:\n",
    "    - image_files (list of str): Paths to the input image files.\n",
    "    - gif_path (str): Path where the output GIF will be saved.\n",
    "    - duration (float): Duration per frame in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - IPython.display.Image object displaying the GIF.\n",
    "    \"\"\"\n",
    "    with imageio.get_writer(gif_path, mode='I', duration=duration) as writer:\n",
    "        for filename in image_files:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    return Image(filename=gif_path)\n",
    "\n",
    "# example usage:\n",
    "# image_files = ['frame_1.jpg', 'frame_middle.jpg', 'frame_2.jpg', 'frame_middle.jpg'] \n",
    "# create_and_display_gif(image_files, gif_path='my_gif.gif', duration=0.5)\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of your solution (short, in bullets) and interpretation of results of strategy A:\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">YOUR EXPLANATION HERE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL OF YOU CODE FOR STATEGY B SHOULD BE IN THIS CELL\n",
    "\n",
    "# REQUIRED OUTPUTS (For each of the input stereo pairs in data/rectified_pairs):\n",
    "# 1. Visualize, side-by-side: left-image | right-image (both with match-points overlaid on them - using small markers)\n",
    "# 2. Visualize, side-by-side: left-image | white-image | right image (with overlayed corresponding subdivisions)\n",
    "# 3. Visualize, by constructing an infinite-loop GIF of the four images: img_1->img_mid->img_2->img_mid->\n",
    " \n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of your solution (short, in bullets) and interpretation of results of strategy B:\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">YOUR EXPLANATION HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Motion Segmentation\n",
    "---\n",
    "\n",
    "### In this section, you are given a pair of images, taken from two different viewpoints, at different times. The captured scene is known to be static, except for a single rigid (non-flexible) object that has moved between the two  captures. Your goal is to 'segment' (classify) the set of matches between the images, as belonging to either 'static' (background) or 'dynamic' (moving object) or 'outliers' (inconsistent).\n",
    "\n",
    "## Instructions\n",
    "* Use only the functions that you had implemented in part 1\n",
    "* Show results for the image pair under the 'data/pairs_to_segment' folder\n",
    "* Capture yourselves two additional image pairs, save them in that same folder, and produce results for them too.\n",
    "\n",
    "\n",
    "## Required Visualization (for each image pair):\n",
    "-- See the 'segmentation' solution outputs under the 'examples' folder\n",
    "* Show the image pair, with 'outlier' matches overlaid (show only a random subset if there are too many)\n",
    "* Show the image pair, with 'static' matches overlaid (show only a random subset if there are too many)\n",
    "* Show the image pair, with 'dynamic' matches overlaid (show only a random subset if there are too many\n",
    "\n",
    "## Implement your solution in the next cell and explain it in short in the following one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ALL OF YOUR CODE FOR PART 3 SHOULD BE IN THIS CELL\n",
    "\n",
    "# REQUIRED OUTPUTS (For each of the input stereo pairs in data/pairs_to_match):\n",
    "# 1. Visualize matches\n",
    "# 2. Visualize epipolar geometry\n",
    "# 3. Visualize matches after constraints\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of your solution (short, in bullets) and interpretation of your results.\n",
    "\n",
    "<span style=\"color: red; font-weight: bold;\">YOUR EXPLANATION HERE</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
